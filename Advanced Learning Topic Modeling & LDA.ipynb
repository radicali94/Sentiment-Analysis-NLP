{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Learning\n",
    "\n",
    "## Topic Modeling & LDA\n",
    "\n",
    "#### Med Radhi Toujani  ----  Med Nacer Cherni  ----  Med Chiheb Bargaoui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply LDA to a set of documents and split them into topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('apple.txt',sep='\\n',index_col=False,header=None);\n",
    "\n",
    "data['headline_text'] = data[0]\n",
    "data['index'] = data.index\n",
    "data=data.drop(columns=[0])\n",
    "documents = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOVE U @APPLE,1.8\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@apple ear pods are AMAZING! Best sound from i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Omg the iPhone 5S is so cool it can read your ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the iPhone 5c is so beautiful &lt;3 @Apple,1.6\\t</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#AttributeOwnership is exactly why @apple will...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Just checked out the specs on the new iOS 7......</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I love the new iOS so much!!!!! Thnx @apple @p...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Can't wait to get my #Iphone5S!!! @apple,1.6\\t</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0                                LOVE U @APPLE,1.8\\t      0\n",
       "1  Thank you @apple, loving my new iPhone 5S!!!!!...      1\n",
       "2  .@apple has the best customer service. In and ...      2\n",
       "3  @apple ear pods are AMAZING! Best sound from i...      3\n",
       "4  Omg the iPhone 5S is so cool it can read your ...      4\n",
       "5      the iPhone 5c is so beautiful <3 @Apple,1.6\\t      5\n",
       "6  #AttributeOwnership is exactly why @apple will...      6\n",
       "7  Just checked out the specs on the new iOS 7......      7\n",
       "8  I love the new iOS so much!!!!! Thnx @apple @p...      8\n",
       "9     Can't wait to get my #Iphone5S!!! @apple,1.6\\t      9"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Radhi\n",
      "[nltk_data]     Toujani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    snowballStemmer = SnowballStemmer(language='english')\n",
    "    return snowballStemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['So', 'glad', '@apple', 'now', 'offers', 'the', 'iPhone', 'in', 'multiple', 'colors', 'because', 'god', 'forbid', 'they', 'actually', 'spend', 'resources', 'improving', 'the', 'network', 'capability.,0.6\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['glad', 'appl', 'offer', 'iphon', 'multipl', 'color', 'forbid', 'actual', 'spend', 'resourc', 'improv', 'network', 'capabl']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 99].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                         [love, appl]\n",
       "1    [thank, appl, love, iphon, appl, iphon, twitte...\n",
       "2                  [appl, best, custom, servic, phone]\n",
       "3             [appl, pod, amaz, best, sound, headphon]\n",
       "4    [iphon, cool, read, finger, print, unlock, iph...\n",
       "5                                [iphon, beauti, appl]\n",
       "6     [exact, appl, appl, market, market, busi, innov]\n",
       "7              [check, spec, wait, updat, bravo, appl]\n",
       "8                       [love, thnx, appl, phillydvib]\n",
       "9                                  [wait, iphon, appl]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 appl\n",
      "1 love\n",
      "2 iphon\n",
      "3 thank\n",
      "4 twitter\n",
      "5 xmhjcu\n",
      "6 best\n",
      "7 custom\n",
      "8 phone\n",
      "9 servic\n",
      "10 amaz\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (10, 1), (13, 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1 (\"iphon\") appears 1 time.\n",
      "Word 10 (\"come\") appears 1 time.\n",
      "Word 13 (\"like\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[200]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.304*\"like\" + 0.262*\"iphon\" + 0.219*\"twitter\" + 0.059*\"thank\" + 0.044*\"fingerprint\" + 0.040*\"love\" + 0.017*\"microsoft\" + 0.015*\"ipad\" + 0.010*\"need\" + 0.008*\"googl\"\n",
      "Topic: 1 \n",
      "Words: 0.394*\"thank\" + 0.213*\"think\" + 0.081*\"time\" + 0.069*\"fingerprint\" + 0.049*\"twitter\" + 0.038*\"microsoft\" + 0.035*\"need\" + 0.021*\"ipad\" + 0.019*\"http\" + 0.016*\"iphon\"\n",
      "Topic: 2 \n",
      "Words: 0.437*\"twitter\" + 0.159*\"http\" + 0.107*\"samsung\" + 0.059*\"love\" + 0.059*\"iphon\" + 0.043*\"time\" + 0.038*\"thank\" + 0.029*\"like\" + 0.017*\"ipad\" + 0.013*\"phone\"\n",
      "Topic: 3 \n",
      "Words: 0.363*\"phone\" + 0.150*\"googl\" + 0.119*\"http\" + 0.098*\"twitter\" + 0.083*\"store\" + 0.080*\"come\" + 0.027*\"samsung\" + 0.025*\"thank\" + 0.015*\"iphon\" + 0.012*\"time\"\n",
      "Topic: 4 \n",
      "Words: 0.250*\"itun\" + 0.240*\"ipodplayerpromo\" + 0.180*\"ipad\" + 0.164*\"ipod\" + 0.144*\"promo\" + 0.006*\"googl\" + 0.005*\"http\" + 0.004*\"microsoft\" + 0.002*\"iphon\" + 0.002*\"go\"\n",
      "Topic: 5 \n",
      "Words: 0.311*\"come\" + 0.146*\"iphon\" + 0.093*\"itun\" + 0.079*\"ipodplayerpromo\" + 0.065*\"ipad\" + 0.051*\"ipod\" + 0.046*\"like\" + 0.044*\"promo\" + 0.037*\"think\" + 0.030*\"http\"\n",
      "Topic: 6 \n",
      "Words: 0.447*\"http\" + 0.181*\"store\" + 0.111*\"iphon\" + 0.093*\"time\" + 0.038*\"phone\" + 0.038*\"need\" + 0.018*\"thank\" + 0.017*\"think\" + 0.015*\"go\" + 0.013*\"come\"\n",
      "Topic: 7 \n",
      "Words: 0.297*\"iphon\" + 0.263*\"need\" + 0.155*\"twitter\" + 0.148*\"think\" + 0.043*\"go\" + 0.032*\"phone\" + 0.010*\"love\" + 0.009*\"itun\" + 0.008*\"store\" + 0.008*\"samsung\"\n",
      "Topic: 8 \n",
      "Words: 0.317*\"microsoft\" + 0.222*\"love\" + 0.197*\"go\" + 0.107*\"googl\" + 0.055*\"store\" + 0.031*\"phone\" + 0.023*\"http\" + 0.010*\"twitter\" + 0.010*\"ipod\" + 0.002*\"iphon\"\n",
      "Topic: 9 \n",
      "Words: 0.635*\"iphon\" + 0.231*\"http\" + 0.041*\"fingerprint\" + 0.018*\"love\" + 0.017*\"samsung\" + 0.014*\"store\" + 0.009*\"twitter\" + 0.009*\"come\" + 0.008*\"ipad\" + 0.006*\"go\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.297*\"love\" + 0.236*\"need\" + 0.141*\"time\" + 0.139*\"twitter\" + 0.053*\"iphon\" + 0.041*\"store\" + 0.019*\"think\" + 0.016*\"microsoft\" + 0.012*\"http\" + 0.008*\"itun\"\n",
      "\n",
      "Topic: 1 \n",
      "Word: 0.428*\"twitter\" + 0.214*\"thank\" + 0.184*\"like\" + 0.049*\"iphon\" + 0.027*\"phone\" + 0.022*\"googl\" + 0.022*\"http\" + 0.012*\"love\" + 0.008*\"store\" + 0.007*\"need\"\n",
      "\n",
      "Topic: 2 \n",
      "Word: 0.268*\"store\" + 0.131*\"fingerprint\" + 0.107*\"go\" + 0.103*\"phone\" + 0.088*\"http\" + 0.084*\"iphon\" + 0.042*\"like\" + 0.031*\"ipad\" + 0.030*\"ipodplayerpromo\" + 0.029*\"itun\"\n",
      "\n",
      "Topic: 3 \n",
      "Word: 0.172*\"ipodplayerpromo\" + 0.168*\"itun\" + 0.144*\"phone\" + 0.116*\"samsung\" + 0.106*\"ipod\" + 0.105*\"promo\" + 0.088*\"ipad\" + 0.053*\"http\" + 0.012*\"store\" + 0.008*\"need\"\n",
      "\n",
      "Topic: 4 \n",
      "Word: 0.442*\"come\" + 0.107*\"ipod\" + 0.093*\"time\" + 0.065*\"itun\" + 0.060*\"ipodplayerpromo\" + 0.055*\"store\" + 0.040*\"http\" + 0.034*\"promo\" + 0.030*\"ipad\" + 0.023*\"iphon\"\n",
      "\n",
      "Topic: 5 \n",
      "Word: 0.169*\"itun\" + 0.125*\"iphon\" + 0.109*\"http\" + 0.082*\"phone\" + 0.072*\"googl\" + 0.058*\"come\" + 0.058*\"microsoft\" + 0.042*\"ipad\" + 0.039*\"store\" + 0.039*\"ipodplayerpromo\"\n",
      "\n",
      "Topic: 6 \n",
      "Word: 0.232*\"twitter\" + 0.205*\"iphon\" + 0.089*\"think\" + 0.089*\"http\" + 0.078*\"thank\" + 0.057*\"ipad\" + 0.051*\"time\" + 0.032*\"phone\" + 0.030*\"come\" + 0.021*\"microsoft\"\n",
      "\n",
      "Topic: 7 \n",
      "Word: 0.230*\"googl\" + 0.206*\"microsoft\" + 0.176*\"think\" + 0.104*\"twitter\" + 0.087*\"go\" + 0.028*\"fingerprint\" + 0.025*\"like\" + 0.019*\"store\" + 0.018*\"samsung\" + 0.017*\"ipad\"\n",
      "\n",
      "Topic: 8 \n",
      "Word: 0.696*\"iphon\" + 0.131*\"http\" + 0.085*\"ipad\" + 0.010*\"ipodplayerpromo\" + 0.010*\"microsoft\" + 0.009*\"itun\" + 0.008*\"thank\" + 0.006*\"ipod\" + 0.006*\"promo\" + 0.005*\"love\"\n",
      "\n",
      "Topic: 9 \n",
      "Word: 0.734*\"http\" + 0.085*\"iphon\" + 0.058*\"need\" + 0.024*\"like\" + 0.019*\"fingerprint\" + 0.014*\"time\" + 0.012*\"love\" + 0.010*\"go\" + 0.008*\"come\" + 0.007*\"twitter\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation: classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glad',\n",
       " 'appl',\n",
       " 'offer',\n",
       " 'iphon',\n",
       " 'multipl',\n",
       " 'color',\n",
       " 'forbid',\n",
       " 'actual',\n",
       " 'spend',\n",
       " 'resourc',\n",
       " 'improv',\n",
       " 'network',\n",
       " 'capabl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5499690771102905\t \n",
      "Topic: 0.635*\"iphon\" + 0.231*\"http\" + 0.041*\"fingerprint\" + 0.018*\"love\" + 0.017*\"samsung\" + 0.014*\"store\" + 0.009*\"twitter\" + 0.009*\"come\" + 0.008*\"ipad\" + 0.006*\"go\"\n",
      "\n",
      "Score: 0.050010502338409424\t \n",
      "Topic: 0.297*\"iphon\" + 0.263*\"need\" + 0.155*\"twitter\" + 0.148*\"think\" + 0.043*\"go\" + 0.032*\"phone\" + 0.010*\"love\" + 0.009*\"itun\" + 0.008*\"store\" + 0.008*\"samsung\"\n",
      "\n",
      "Score: 0.05000915005803108\t \n",
      "Topic: 0.304*\"like\" + 0.262*\"iphon\" + 0.219*\"twitter\" + 0.059*\"thank\" + 0.044*\"fingerprint\" + 0.040*\"love\" + 0.017*\"microsoft\" + 0.015*\"ipad\" + 0.010*\"need\" + 0.008*\"googl\"\n",
      "\n",
      "Score: 0.05000494420528412\t \n",
      "Topic: 0.311*\"come\" + 0.146*\"iphon\" + 0.093*\"itun\" + 0.079*\"ipodplayerpromo\" + 0.065*\"ipad\" + 0.051*\"ipod\" + 0.046*\"like\" + 0.044*\"promo\" + 0.037*\"think\" + 0.030*\"http\"\n",
      "\n",
      "Score: 0.05000383034348488\t \n",
      "Topic: 0.447*\"http\" + 0.181*\"store\" + 0.111*\"iphon\" + 0.093*\"time\" + 0.038*\"phone\" + 0.038*\"need\" + 0.018*\"thank\" + 0.017*\"think\" + 0.015*\"go\" + 0.013*\"come\"\n",
      "\n",
      "Score: 0.05000193789601326\t \n",
      "Topic: 0.437*\"twitter\" + 0.159*\"http\" + 0.107*\"samsung\" + 0.059*\"love\" + 0.059*\"iphon\" + 0.043*\"time\" + 0.038*\"thank\" + 0.029*\"like\" + 0.017*\"ipad\" + 0.013*\"phone\"\n",
      "\n",
      "Score: 0.05000032112002373\t \n",
      "Topic: 0.363*\"phone\" + 0.150*\"googl\" + 0.119*\"http\" + 0.098*\"twitter\" + 0.083*\"store\" + 0.080*\"come\" + 0.027*\"samsung\" + 0.025*\"thank\" + 0.015*\"iphon\" + 0.012*\"time\"\n",
      "\n",
      "Score: 0.05000016838312149\t \n",
      "Topic: 0.394*\"thank\" + 0.213*\"think\" + 0.081*\"time\" + 0.069*\"fingerprint\" + 0.049*\"twitter\" + 0.038*\"microsoft\" + 0.035*\"need\" + 0.021*\"ipad\" + 0.019*\"http\" + 0.016*\"iphon\"\n",
      "\n",
      "Score: 0.05000004917383194\t \n",
      "Topic: 0.250*\"itun\" + 0.240*\"ipodplayerpromo\" + 0.180*\"ipad\" + 0.164*\"ipod\" + 0.144*\"promo\" + 0.006*\"googl\" + 0.005*\"http\" + 0.004*\"microsoft\" + 0.002*\"iphon\" + 0.002*\"go\"\n",
      "\n",
      "Score: 0.050000011920928955\t \n",
      "Topic: 0.317*\"microsoft\" + 0.222*\"love\" + 0.197*\"go\" + 0.107*\"googl\" + 0.055*\"store\" + 0.031*\"phone\" + 0.023*\"http\" + 0.010*\"twitter\" + 0.010*\"ipod\" + 0.002*\"iphon\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[99]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic that our model assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation: classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5499815940856934\t \n",
      "Topic: 0.696*\"iphon\" + 0.131*\"http\" + 0.085*\"ipad\" + 0.010*\"ipodplayerpromo\" + 0.010*\"microsoft\" + 0.009*\"itun\" + 0.008*\"thank\" + 0.006*\"ipod\" + 0.006*\"promo\" + 0.005*\"love\"\n",
      "\n",
      "Score: 0.050006456673145294\t \n",
      "Topic: 0.232*\"twitter\" + 0.205*\"iphon\" + 0.089*\"think\" + 0.089*\"http\" + 0.078*\"thank\" + 0.057*\"ipad\" + 0.051*\"time\" + 0.032*\"phone\" + 0.030*\"come\" + 0.021*\"microsoft\"\n",
      "\n",
      "Score: 0.050003595650196075\t \n",
      "Topic: 0.169*\"itun\" + 0.125*\"iphon\" + 0.109*\"http\" + 0.082*\"phone\" + 0.072*\"googl\" + 0.058*\"come\" + 0.058*\"microsoft\" + 0.042*\"ipad\" + 0.039*\"store\" + 0.039*\"ipodplayerpromo\"\n",
      "\n",
      "Score: 0.0500025749206543\t \n",
      "Topic: 0.734*\"http\" + 0.085*\"iphon\" + 0.058*\"need\" + 0.024*\"like\" + 0.019*\"fingerprint\" + 0.014*\"time\" + 0.012*\"love\" + 0.010*\"go\" + 0.008*\"come\" + 0.007*\"twitter\"\n",
      "\n",
      "Score: 0.050002504140138626\t \n",
      "Topic: 0.268*\"store\" + 0.131*\"fingerprint\" + 0.107*\"go\" + 0.103*\"phone\" + 0.088*\"http\" + 0.084*\"iphon\" + 0.042*\"like\" + 0.031*\"ipad\" + 0.030*\"ipodplayerpromo\" + 0.029*\"itun\"\n",
      "\n",
      "Score: 0.05000146105885506\t \n",
      "Topic: 0.297*\"love\" + 0.236*\"need\" + 0.141*\"time\" + 0.139*\"twitter\" + 0.053*\"iphon\" + 0.041*\"store\" + 0.019*\"think\" + 0.016*\"microsoft\" + 0.012*\"http\" + 0.008*\"itun\"\n",
      "\n",
      "Score: 0.05000137537717819\t \n",
      "Topic: 0.428*\"twitter\" + 0.214*\"thank\" + 0.184*\"like\" + 0.049*\"iphon\" + 0.027*\"phone\" + 0.022*\"googl\" + 0.022*\"http\" + 0.012*\"love\" + 0.008*\"store\" + 0.007*\"need\"\n",
      "\n",
      "Score: 0.0500003807246685\t \n",
      "Topic: 0.442*\"come\" + 0.107*\"ipod\" + 0.093*\"time\" + 0.065*\"itun\" + 0.060*\"ipodplayerpromo\" + 0.055*\"store\" + 0.040*\"http\" + 0.034*\"promo\" + 0.030*\"ipad\" + 0.023*\"iphon\"\n",
      "\n",
      "Score: 0.05000007152557373\t \n",
      "Topic: 0.230*\"googl\" + 0.206*\"microsoft\" + 0.176*\"think\" + 0.104*\"twitter\" + 0.087*\"go\" + 0.028*\"fingerprint\" + 0.025*\"like\" + 0.019*\"store\" + 0.018*\"samsung\" + 0.017*\"ipad\"\n",
      "\n",
      "Score: 0.05000000819563866\t \n",
      "Topic: 0.172*\"ipodplayerpromo\" + 0.168*\"itun\" + 0.144*\"phone\" + 0.116*\"samsung\" + 0.106*\"ipod\" + 0.105*\"promo\" + 0.088*\"ipad\" + 0.053*\"http\" + 0.012*\"store\" + 0.008*\"need\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[99]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model on an unknown document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5499909520149231\t Topic: 0.437*\"twitter\" + 0.159*\"http\" + 0.107*\"samsung\" + 0.059*\"love\" + 0.059*\"iphon\"\n",
      "Score: 0.050004422664642334\t Topic: 0.363*\"phone\" + 0.150*\"googl\" + 0.119*\"http\" + 0.098*\"twitter\" + 0.083*\"store\"\n",
      "Score: 0.050003502517938614\t Topic: 0.635*\"iphon\" + 0.231*\"http\" + 0.041*\"fingerprint\" + 0.018*\"love\" + 0.017*\"samsung\"\n",
      "Score: 0.05000048130750656\t Topic: 0.297*\"iphon\" + 0.263*\"need\" + 0.155*\"twitter\" + 0.148*\"think\" + 0.043*\"go\"\n",
      "Score: 0.05000028386712074\t Topic: 0.447*\"http\" + 0.181*\"store\" + 0.111*\"iphon\" + 0.093*\"time\" + 0.038*\"phone\"\n",
      "Score: 0.05000006780028343\t Topic: 0.304*\"like\" + 0.262*\"iphon\" + 0.219*\"twitter\" + 0.059*\"thank\" + 0.044*\"fingerprint\"\n",
      "Score: 0.05000006780028343\t Topic: 0.394*\"thank\" + 0.213*\"think\" + 0.081*\"time\" + 0.069*\"fingerprint\" + 0.049*\"twitter\"\n",
      "Score: 0.05000006780028343\t Topic: 0.250*\"itun\" + 0.240*\"ipodplayerpromo\" + 0.180*\"ipad\" + 0.164*\"ipod\" + 0.144*\"promo\"\n",
      "Score: 0.05000006780028343\t Topic: 0.311*\"come\" + 0.146*\"iphon\" + 0.093*\"itun\" + 0.079*\"ipodplayerpromo\" + 0.065*\"ipad\"\n",
      "Score: 0.05000006780028343\t Topic: 0.317*\"microsoft\" + 0.222*\"love\" + 0.197*\"go\" + 0.107*\"googl\" + 0.055*\"store\"\n"
     ]
    }
   ],
   "source": [
    "unknown_document = 'Samsung galaxy is the best'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unknown_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
